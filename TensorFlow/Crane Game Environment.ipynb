{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e9a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_agents.environments.py_environment\n",
    "import tf_agents.specs.array_spec\n",
    "import tf_agents.trajectories.time_step as ts\n",
    "import random\n",
    "import numpy as np\n",
    "class BallSortCraneEnvironment(tf_agents.environments.py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        # Define constants for the environment\n",
    "        self.SCREEN_WIDTH = 800\n",
    "        self.SCREEN_HEIGHT = 600  # Define the screen height\n",
    "        self.BAR_WIDTH = 20\n",
    "        self.BAR_HEIGHT = 100\n",
    "        self.BALL_RADIUS = 20\n",
    "        self.BASKET_WIDTH = 60\n",
    "        self.BASKET_HEIGHT = 20\n",
    "        self.MOVEMENT_SPEED = 5\n",
    "        self.VELOCITY = 10\n",
    "\n",
    "        # Initialize the game state variables\n",
    "        self.bar_x = self.SCREEN_WIDTH - (self.SCREEN_WIDTH // 4)\n",
    "        self.bar_y = self.SCREEN_HEIGHT - (self.SCREEN_HEIGHT // 4)\n",
    "        self.bar_height = self.BAR_HEIGHT\n",
    "        self.score = 0\n",
    "        self.is_grabbing = False  # Initialize grabbing state\n",
    "        self.is_holding_ball = False  # Initialize ball holding state\n",
    "        self.randomize_positions()\n",
    "        self.previous_distance_to_ball = float('inf')  \n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self._action_spec = tf_agents.specs.array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=3)\n",
    "        self._observation_spec = tf_agents.specs.array_spec.ArraySpec(\n",
    "            shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        # Reset the game state to the initial state\n",
    "        self.bar_x = self.SCREEN_WIDTH - (self.SCREEN_WIDTH // 4)\n",
    "        self.bar_y = self.SCREEN_HEIGHT - (self.SCREEN_HEIGHT // 4)\n",
    "        self.bar_height = self.BAR_HEIGHT\n",
    "        self.score = 0\n",
    "        self.is_grabbing = False\n",
    "        self.is_holding_ball = False\n",
    "        self.randomize_positions()\n",
    "\n",
    "        # Return initial observation as a TimeStep\n",
    "        return ts.restart(tf.constant(self._normalize_observation(self.get_observation()), dtype=np.float32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        # Execute the action in the environment and return the next TimeStep\n",
    "        if action == 0:  # Move left\n",
    "            self.bar_x = max(self.bar_x - self.MOVEMENT_SPEED, 0)\n",
    "        elif action == 1:  # Move right\n",
    "            self.bar_x = min(self.bar_x + self.MOVEMENT_SPEED, self.SCREEN_WIDTH)\n",
    "        elif action == 2:  # Extend bar\n",
    "            self.bar_height = min(self.bar_height + 10, self.SCREEN_HEIGHT)\n",
    "        elif action == 3:  # Shrink bar\n",
    "            self.bar_height = max(self.bar_height - 10, 0)\n",
    "        \n",
    "        # Implement game logic (e.g., ball movement, collision checking)\n",
    "        self.update_game_state()\n",
    "        \n",
    "        self.previous_distance_to_ball = abs(self.bar_x - self.ball_x)\n",
    "        # Calculate the reward and check the termination condition\n",
    "        reward, done = self.calculate_reward_and_termination()\n",
    "\n",
    "        if done:\n",
    "            # Cast the reward to the appropriate data type (e.g., float32)\n",
    "            reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "            return ts.termination(\n",
    "                tf.constant(self._normalize_observation(self.get_observation()), dtype=np.float32),\n",
    "                reward\n",
    "            )\n",
    "        else:\n",
    "            # Cast the reward to the appropriate data type (e.g., float32)\n",
    "            reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "            return ts.transition(\n",
    "                tf.constant(self._normalize_observation(self.get_observation()), dtype=np.float32),\n",
    "                reward)\n",
    "    def update_game_state(self):\n",
    "        # Implement game state update logic here\n",
    "        self.ball_x += 0  # Implement ball movement\n",
    "        self.ball_y -= self.VELOCITY  # Simulate ball falling\n",
    "\n",
    "        # Check if the ball collides with the bar\n",
    "        if (\n",
    "            self.bar_x - self.BAR_WIDTH / 2 < self.ball_x < self.bar_x + self.BAR_WIDTH / 2\n",
    "            and self.bar_y - self.bar_height <= self.ball_y + self.BALL_RADIUS\n",
    "        ):\n",
    "            self.score += 1\n",
    "            \n",
    "    def randomize_positions(self):\n",
    "        # Randomize the positions of the ball and basket within valid ranges\n",
    "        self.ball_x = random.randint(self.BALL_RADIUS, self.SCREEN_WIDTH // 2 - self.BALL_RADIUS)\n",
    "        self.basket_x = random.randint(\n",
    "            self.SCREEN_WIDTH // 2 + self.BASKET_WIDTH // 2,\n",
    "            self.SCREEN_WIDTH - self.BASKET_WIDTH // 2\n",
    "        )\n",
    "        # Ensure valid y-positions for the ball and basket\n",
    "        max_pillar_height = self.SCREEN_HEIGHT // 2\n",
    "        self.ball_y = random.randint(self.BALL_RADIUS, max_pillar_height - self.BALL_RADIUS)\n",
    "        self.basket_y = self.BASKET_HEIGHT // 2\n",
    "\n",
    "    def calculate_reward_and_termination(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Calculate the current distance to the ball\n",
    "        distance_to_ball = abs(self.bar_x - self.ball_x)\n",
    "\n",
    "        if not self.is_grabbing:\n",
    "            # Reward for moving closer to the ball, scaled by a factor\n",
    "            reward += max(0, (self.previous_distance_to_ball - distance_to_ball) * 0.1)\n",
    "        else:\n",
    "            # When the ball is grabbed, check for successful delivery or dropping\n",
    "            if self.is_holding_ball and self.ball_in_basket():\n",
    "                reward += successful_delivery_reward  # Reward for delivering the ball to the basket\n",
    "                done = True\n",
    "            elif self.ball_falls():\n",
    "                reward -= ball_fall_penalty  # Penalty for dropping the ball\n",
    "                done = True\n",
    "\n",
    "        # Update the previous distance to the ball for the next step\n",
    "        self.previous_distance_to_ball = distance_to_ball\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def ball_in_basket(self):\n",
    "        # Check if the ball is within the basket\n",
    "        return (\n",
    "            self.ball_x + self.BALL_RADIUS >= self.basket_x - self.BASKET_WIDTH / 2 and\n",
    "            self.ball_x - self.BALL_RADIUS <= self.basket_x + self.BASKET_WIDTH / 2 and\n",
    "            self.ball_y - self.BALL_RADIUS <= self.basket_y + self.BASKET_HEIGHT / 2\n",
    "        )\n",
    "\n",
    "    def ball_falls(self):\n",
    "        # Check if the ball falls out of the screen\n",
    "        return self.ball_y < -self.BALL_RADIUS\n",
    "\n",
    "\n",
    "    def get_observation(self):\n",
    "        # Generate the observation for the agent\n",
    "        observation = [\n",
    "            self.bar_x, self.bar_y, self.bar_height, self.score,\n",
    "            self.ball_x, self.ball_y, self.basket_x, self.basket_y\n",
    "        ]\n",
    "        return observation\n",
    "\n",
    "    def _normalize_observation(self, observation):\n",
    "        # Normalize observation values to the range [0, 1]\n",
    "        normalized_observation = []\n",
    "        max_values = [self.SCREEN_WIDTH, self.SCREEN_HEIGHT, self.SCREEN_HEIGHT, float('inf'), \n",
    "                      self.SCREEN_WIDTH, self.SCREEN_HEIGHT, self.SCREEN_WIDTH, self.SCREEN_HEIGHT]\n",
    "        for obs, max_val in zip(observation, max_values):\n",
    "            normalized_observation.append(obs / max_val)\n",
    "        return normalized_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import utils\n",
    "\n",
    "# Create the BallSortCrane environment\n",
    "train_env = BallSortCraneEnvironment()\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "\n",
    "# Define the Q-network\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "# Define the DQN agent\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "# Define the replay buffer\n",
    "replay_buffer_capacity = 100\n",
    "replay_buffer = tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "# Define the data collection policy\n",
    "epsilon = tf.constant(0.1, dtype=np.float32)  # Specify epsilon as a TensorFlow constant\n",
    "collect_policy = tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "    policy=agent.policy,\n",
    "    epsilon=epsilon)\n",
    "\n",
    "# Define the data collection driver\n",
    "collect_driver = tf_agents.drivers.dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=1)\n",
    "\n",
    "# Define the training metrics\n",
    "train_metrics = [\n",
    "    tf_agents.metrics.tf_metrics.AverageReturnMetric(),\n",
    "    tf_agents.metrics.tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "# Define the training and evaluation loops\n",
    "num_iterations = 10 #10000\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "batch_size = 1\n",
    "num_eval_episodes = 10\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_driver.run()\n",
    "\n",
    "# Create a dataset from the replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Define the training loop\n",
    "def train_agent(num_iterations):\n",
    "    for _ in range(num_iterations):\n",
    "        # Collect a step using the collect_driver\n",
    "        collect_driver.run()\n",
    "\n",
    "        # Sample a batch of data from the replay buffer\n",
    "        experience, unused_info = next(iterator)\n",
    "\n",
    "        # Train the agent\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        # Log training metrics\n",
    "        if agent.train_step_counter.numpy() % 100 == 0:\n",
    "            for metric in train_metrics:\n",
    "                metric.tf_summaries(\n",
    "                    train_step=agent.train_step_counter, step_metrics=metric.result())\n",
    "            # Print a checkpoint\n",
    "            print(f\"Iteration {iteration}, Loss: {train_loss.numpy()}, Metrics: {train_metrics}\")\n",
    "\n",
    "# Train the agent\n",
    "train_agent(num_iterations)\n",
    "\n",
    "# Evaluate the agent\n",
    "eval_env = BallSortCraneEnvironment()\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "print(\"Average Return:\", avg_return.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1328436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfe2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_reward_and_termination(self):\n",
    "        # Default reward and termination conditions\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Check if the agent is grabbing the ball\n",
    "        if self.is_grabbing:\n",
    "            if not self.is_holding_ball:  # Check if the agent is not already holding the ball\n",
    "                # Reward the agent for grabbing the ball initially\n",
    "                reward += 0.5  # Higher initial reward\n",
    "\n",
    "                # Set the holding ball status to True\n",
    "                self.is_holding_ball = True\n",
    "        else:\n",
    "            if self.is_holding_ball:\n",
    "                # Check if the ball is not in the basket while releasing\n",
    "                if (\n",
    "                    not (\n",
    "                        self.ball_x + self.BALL_RADIUS >= self.basket_x - self.BASKET_WIDTH / 2\n",
    "                        and self.ball_x - self.BALL_RADIUS <= self.basket_x + self.BASKET_WIDTH / 2\n",
    "                        and self.ball_y - self.BALL_RADIUS <= self.basket_y + self.BASKET_HEIGHT / 2\n",
    "                    )\n",
    "                ):\n",
    "                    # Penalize the agent for dropping the ball outside the basket\n",
    "                    reward -= 0.5  # Higher penalty\n",
    "\n",
    "                # Reset the holding ball status\n",
    "                self.is_holding_ball = False\n",
    "\n",
    "        # Check if the ball is in the basket\n",
    "        if (\n",
    "            self.ball_x + self.BALL_RADIUS >= self.basket_x - self.BASKET_WIDTH / 2\n",
    "            and self.ball_x - self.BALL_RADIUS <= self.basket_x + self.BASKET_WIDTH / 2\n",
    "            and self.ball_y - self.BALL_RADIUS <= self.basket_y + self.BASKET_HEIGHT / 2\n",
    "        ):\n",
    "            reward += 1  # Positive reward for catching the ball\n",
    "            done = True  # Terminate the episode when the ball is caught\n",
    "\n",
    "        # Check if the ball falls out of the screen\n",
    "        if self.ball_y < -self.BALL_RADIUS:\n",
    "            reward -= 1  # Negative reward for letting the ball fall\n",
    "            done = True  # Terminate the episode when the ball falls\n",
    "\n",
    "        return reward, done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
